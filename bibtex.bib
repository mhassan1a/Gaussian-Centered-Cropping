@unpublished{RnDHassan,
    abstract = {In the contemporary landscape of computer vision and deep learning (DL), contrastive learning (CL) stands out as one of the most important self-supervised learning (SSL) frameworks. CL offers the advantage of learning directly from unlabeled data by leveraging fundamental knowledge representation principles, thereby enabling DL models to learn features that are transferable to downstream tasks. However, the success of CL methods is highly dependent on the effective use of robust image augmentation techniques, particularly image cropping. CL methods utilize randomized cropping (RC) to produce semantically related views (i.e. positive pairs) that serve as self-labels. However, RC can also introduce false positives, where views from different classes are incorrectly treated as positive pairs, significantly degrading performance.\\

Consequently, the primary challenge associated with image cropping, in the context of CL, lies in striking a balance between introducing non-trivial positive pairs and minimizing the occurrence of false positives. To address this challenge, this project proposes a novel approach: A parameterized cropping method, Gaussian-Centered Cropping (GCC), that facilitates the fine-tuning of the cropping process to reduce the likelihood of false positives and improve the performance. The experimental results demonstrate that the proposed method outperforms Random Crop by 2.7, 6.7, 9.5, and 12.4 percentage points for crop sizes of 20\%, 40\%, 60\%, and 80\% respectively, at the same computational cost. In addition, an enhanced version of GCC, Multi-object Gaussian-Centered Cropping (MGCC), is presented to handle images containing multiple objects.},
    title = {The Effect of the Randomness Underlying Image Cropping in Contrastive Learning: A Comparative Study},
    author = {Hassan, Mohamed},
    year = {2024},
    month = {August},
    note = {SS23 Houben, Wasil supervising}
}
